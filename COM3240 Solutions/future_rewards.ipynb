{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3c2f95-d483-424f-894b-44bb7349a85a",
   "metadata": {},
   "source": [
    "# Future Rewards\n",
    "\n",
    "We have seen how to estimate expectations for immediate rewards in the concept of bandits. However, these algorithms are unable to capture reward expectations in scenarios where rewards are obtained after a sequence of actions. In other words, the reward for specific actions will arrive only in the future. Take, for example, the game of chess: winning only comes after a number of strategic actions. While the player will attempt to make the best move each time, most actions are only indirectly linked to rewards. These scenarios are more general than bandits and require an appropriate framework for describing them. We need to know the environment's states, how these states are connected, which actions are available, and what the probability of reaching another state by taking an action is. This framework, known as a Markov Decision Process (MDP), is an essential intermediate step in understanding the derivation of key reinforcement learning algorithms. In what follows, we will discuss MDPs and the Bellman Equation, an important mathematical derivation underpinning the Temporal Difference Learning algorithms, applicable to **future rewards**.\n",
    "\n",
    "## Markov Decision Process\n",
    "\n",
    "In a **Markov Decision Process (MDP)**, the concept of **'state'** represents the current situation or position in which the system or agent finds itself. This encapsulates all the relevant information required to make future decisions. An **'action'** is an operation or move that the agent can perform within a given state, which leads to a transition to a new state or states. The decision-making process in an MDP involves selecting actions in various states based on a policy that seeks to maximize some notion of cumulative reward over time. Diagrams commonly used to represent MDPs depict states as circles, while actions are represented by smaller solid circles. Arrows extending from these action circles indicate transitions to the next states. Importantly, when taking an action, the probabilities of transitioning to different possible next states must sum up to 1. This ensures that the model accurately reflects the probabilistic nature of the environment, where the outcome of an action is uncertain but the total likelihood of ending up in any of the possible next states is certain.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider an analog safe. For simplicity, we will assume that opening it requires turning the analog dial twice, first pointing to a number $ x_1 $ and then to a number $ x_2 $. We will assume the dial has options for only two numbers (say 0 and 1). This would obviously be a very unsafe safe, but we only need to demonstrate the basic concept here.\n",
    "\n",
    "Starting from state $ S0 $, if the correct numbers $ x_1 $ and $ x_2 $ are entered, we receive $ r=1 $ (the safe opens). Otherwise, we receive a reward of 0. We assume the safe will not become 'locked' after an unsuccessful attempt to open it. This scenario is deterministic and corresponds to the following diagram.\n",
    "\n",
    "We denote the first action as $ A1 $, which can take two values (correct or incorrect), leading to state $ S1 $ if the correct number $ x_1 $ is given and to $ S2 $ if the incorrect number is given. From $ S1 $, one ends up in terminal state $ T1 $ with reward $ r=1 $ if $ A2 $ selects the correct number $ x_2 $; otherwise, we end up in terminal state $ T2 $ with reward $r=0$. Similarly, $S2$ ends up in the unrewarded terminal state $T2$ or $T3$, where $T2$ corresponds to partially incorrect guesses and $T3$ entirely incorrect guesses. All intermediate steps also give reward $r=0$.\n",
    "\n",
    "One could also use a slightly different mapping with more terminal states, but we selected this option as simple enough while depicting all possible intermediate actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10497832",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.dropbox.com/scl/fi/pdjmcpcpszgf5x3zn4gjj/MDP1.png?rlkey=5o3onjifzy825iol4zxspe9u8&raw=1\" width=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119111d6-4ff2-42af-a1b5-f4afa9a40fcf",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "In the example of the safe, now assume that there is a problem with the mechanism, and 10% of the times, while you have selected the correct digit, the mechanism slips and an incorrect digit is internally selected. Update the diagram. **Hint:** This is a stochastic scenario. From every correct action, two arrows will be departing. You will need to use the symbol below, which denotes that by taking an action $a$ you may end up in many different states. The transition probabilities to these states, however, should sum up to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db1c51-50e3-4414-86bb-c95369c4a5a9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.dropbox.com/scl/fi/8jxeo9otc4sitkgp13t95/MDP2.png?rlkey=aqxodbclh908mlxz54qp7snvm&raw=1\" width=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55efc56",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.dropbox.com/scl/fi/d8ekcgsbqall0mwuuz2cz/MDP3.png?rlkey=6x459zx6r21x88xhgpem8d6j8&raw=1\" width=\"600\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951606c9-094e-477e-a533-3fac890062c4",
   "metadata": {},
   "source": [
    "## Markovian Property\n",
    "\n",
    "The Markovian property, or memorylessness, signifies that the future state of a process depends only on the current state and not on the sequence of events that preceded it. This property ensures that the probability of transitioning to any future state from the current state can be determined solely based on the current state and the action taken.\n",
    "\n",
    "For a Markov process with states , the property can be mathematically expressed as:\n",
    "\n",
    "$$ P(S_{n+1} = s'|S_n = s, S_{n-1} = s_{n-1}, ..., S_0 = s_0) = P(S_{n+1} = s'|S_n = s) $$\n",
    "\n",
    "This equation signifies that the probability of transitioning to the next state  depends solely on the current state  and is independent of all previous states.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the simple act of rolling a fair six-sided die. The probability of rolling a six on any given toss is 1/6, regardless of the outcomes of previous rolls. This scenario demonstrates the Markovian property because the outcome of the current roll does not depend on the history of rolls. Each roll is an independent event, and the 'state' can be considered as the outcome of the current roll.\n",
    "\n",
    "### State Augmentation\n",
    "\n",
    "Transitioning to a more complex example, in the board game Monopoly, if we consider the state to be solely the location of a pawn on the board, this model would not be Markovian. The reason is that future actions and their outcomes (e.g., paying rent, buying properties) depend not only on the pawn's location but also on the player's available money or purchased property, which is a result of past actions.\n",
    "\n",
    "However, by employing 'state augmentation', we can include the player's money and property ownership as part of the state. This augmented state now encapsulates both the pawn's location and the player's financial status, making the system Markovian. With this broader definition of state, the future actions and their outcomes depend solely on this augmented state, adhering to the Markovian property. The probability of any future transition is determined by the current augmented state and the actions taken, ensuring that past actions influence future possibilities only through their effect on the current state.\n",
    "\n",
    "### Real Life and RL\n",
    "\n",
    "In real life, and by extension in many practical issues Reinforcement Learning (RL) tries to solve, few things are strictly Markovian. The past frequently has an effect on future actions, a reflection of the complex interdependencies and the accumulation of consequences over time. This reality makes state augmentation not just a useful but an essential technique in RL. By incorporating relevant historical information into the current state, state augmentation allows RL models to navigate and learn from environments where future states and rewards are not solely dependent on the immediate preceding state but also on the history leading up to it. This approach allows us to apply RL in complex, dynamic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf43b8-2636-4237-81ea-489e11c9492d",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "For each of the following scenarios, define an appropriate augmented state that makes the system Markovian.\n",
    "\n",
    "1. **Temperature Control System**: You're designing a thermostat to control the temperature of a room. The system decides whether to turn the heating on or off based on the current temperature. However, the effectiveness of the heater also depends on the outside temperature, which is not considered in the current state.\n",
    "\n",
    "2. **Text-Based Game Navigation**: In a text-based adventure game, a player moves between rooms based on descriptions given by the system. The action to move back (\"go back\") to the previously visited room doesn't always lead to the same room, as the path is not linear but depends on the sequence of rooms visited.Assume that it depends in the previous two rooms.\n",
    "\n",
    "3. **Stock Trading Strategy**: A trading algorithm decides to buy, sell, or hold based on the current price of a stock. However, the decision also depends of the trend over the last three days.\n",
    "\n",
    "4. **Battery Charging Station**: An autonomous vehicle decides when to charge based on its current battery level. However, the optimal decision also depends on the vehicle's next task's energy requirements and the distance to the next charging station.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac43f37-05c0-41e3-b0d5-7c1d6575a68c",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "1. **Temperature Control System**: Augment the state to include both the current room temperature and the outside temperature. State = (CurrentRoomTemperature, OutsideTemperature).\n",
    "\n",
    "2. **Text-Based Game Navigation**: Augment the state to include a history of rooms visited, at least the last two rooms, to make \"go back\" deterministic. State = (CurrentRoom, PreviousRoom 1, PreviousRoom 2).\n",
    "\n",
    "3. **Stock Trading Strategy**: Augment the state to include the stock price at the current time step and the prices at the last three time steps. State = (PriceToday, PriceDayMinus1, PriceDayMinus2, PriceDayMinus3).\n",
    "\n",
    "4. **Battery Charging Station**: Augment the state to include the current battery level, the energy requirement for the next task, and the distance to the next charging station. State = (CurrentBatteryLevel, NextTaskEnergyRequirement, DistanceToNextChargingStation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2754a69-af3e-43bc-9db0-cf417889548a",
   "metadata": {},
   "source": [
    "All these examples clearly define how history affects the current status of the agent, making state augmentation easy to formalise. In practical situations, however, it is not always clear how much of the past influences a problem, and when modelling, we have to make an assumption and test the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf918f-dbeb-474b-9009-6850aa5cb240",
   "metadata": {},
   "source": [
    "# Action-value functions\n",
    "\n",
    "## Expected Total Return\n",
    "In the context of future rewards, we start from a state $s$ with the goal to select actions that maximise the expected return, i.e., the average total reward across many episodes of playing the game. The agent chooses actions $a$ based on policy $\\pi(a|s)$; we have previously covered this under bandits. Here, we extend these concepts to a multi-state scenario, interpreting them as probabilities conditioned on the state $s$. The policies aim to maximize the agent's total expected return over time, accounting for the uncertain and probabilistic nature of the environment.\n",
    "\n",
    "In the simplest case where the episode lasts a fixed time $T$, we can simply write:\n",
    "\n",
    "$$ G_t = R_{t+1}+ R_{t+2}+ R_{t+3}+\\ldots +R_{T}.$$\n",
    "\n",
    "However, in many scenarios, the number of steps is not fixed. We consider a discount factor $\\gamma$ with $0 \\leq \\gamma < 1$, and define $G$ as the sum of discounted rewards:\n",
    "\n",
    "$$ G_t = R_{t+1}+ \\gamma R_{t+2}+ \\gamma^2 R_{t+3}+\\ldots $$\n",
    "\n",
    "The fact that $\\gamma < 1$ means that after some number of $k$ steps, depending on $\\gamma$, $\\gamma^{k-1} << 1$ and thus becomes negligible; in practice we optimise within a finite window of future actions, as some future rewards will appear insignificant. The discount factor $\\gamma$ prioritises immediate rewards over distant future rewards, reflecting the inherent uncertainty of those future rewards and the agent's preference for immediate gain. This second equation can also be applied in scenarios with fixed steps, thus it is the more general of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2761c9f-afc2-417d-9047-cacf11bb407a",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Imagine a garbage collecting robot at a junction with two paths: it can turn right ($G^{tr}_0$) or turn left ($G^{tl}_0$). This example demonstrates the calculation of the expected return, $G_0$, from the starting position for both choices, using a discount factor of $\\gamma = 0.9$.\n",
    "\n",
    "#### Scenario\n",
    "\n",
    "The sequences of rewards for turning right ($R^{tr}$) or left ($R^{tl}$) are represented as follows:\n",
    "\n",
    "- **Right Path Rewards Sequence**: The sequence is $[+10, +10, +10, 0, 0, 0, 0, -50, 0, 0, 0, 0]$. \n",
    "  - This corresponds to $R^{tr}_1 = +10$, $R^{tr}_2 = +10$, $R^{tr}_3 = +10$, with subsequent zeros until $R^{tr}_8 = -50$, followed by zeros, representing the rewards the robot collects at each step when it turns right. The $R^{tr}_8=-50$ is because there is a hole on the road causing the robot some damage.\n",
    "\n",
    "- **Left Path Rewards Sequence**: The sequence is $[+2, +2, +2, 0, 0, 0, +2, +2, +2, +2, +2, +2]$.\n",
    "  - This corresponds to $R^{tl}_1 = +2$, $R^{tl}_2 = +2$, $R^{tl}_3 = +2$, with subsequent zeros and then $R^{tl}_7$ to $R^{tl}_{12} = +2$ each, representing the rewards the robot collects at each step when it turns left.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21186f-5ebe-4d9c-961e-2f44c43bbfc8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.dropbox.com/scl/fi/8rv29tb9zta1kc8evf3to/MDP4.png?rlkey=azqeri6wffvrsga6h2kppld8l&raw=1\" width=\"600\" />\n",
    "</div>\n",
    "Note: In this diagra, we ommited denoting explicitly the action as $a$ for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db6dac-d15e-438f-8c25-f764caccab94",
   "metadata": {},
   "source": [
    "#### Calculation for $\\gamma = 0.9$\n",
    "\n",
    "To calculate the expected return $G_0$ from the starting position if the robot turns right ($G^{tr}_0$) and left ($G^{tl}_0$), we explicitly sum the rewards, weighted by $\\gamma$ raised to the power of the step index:\n",
    "\n",
    "- For the **Right Path** ($G^{tr}_0$):\n",
    "\n",
    "$$\n",
    "G^{tr}_0 = R^{tr}_1 + \\gamma R^{tr}_2 + \\gamma^2 R^{tr}_3 + \\ldots + \\gamma^{7} R^{tr}_{8} + \\gamma^{8} R^{tr}_{9} + \\gamma^{9} R^{tr}_{10} + \\gamma^{10} R^{tr}_{11} + \\gamma^{11} R^{tr}_{12}\n",
    "$$\n",
    "\n",
    "After performing the calculation:\n",
    "\n",
    "$$\n",
    "G^{tr}_0 \\approx 3.19\n",
    "$$\n",
    "\n",
    "- For the **Left Path** ($G^{tl}_0$):\n",
    "\n",
    "$$\n",
    "G^{tl}_0 = R^{tl}_1 + \\gamma R^{tl}_2 + \\gamma^2 R^{tl}_3 + \\ldots + \\gamma^{11} R^{tl}_{12}\n",
    "$$\n",
    "\n",
    "After performing the calculation:\n",
    "\n",
    "$$\n",
    "G^{tl}_0 \\approx 10.40\n",
    "$$\n",
    "\n",
    "Therefore the path leading to the higest reward is the **Left Path**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c3821-326a-4993-8632-4dbf9ff8824c",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Study the scenario and calculations in the example above. Assume now that $\\gamma = 0.5$. Which action seems better for the robot now, turning right or turning left? Calculate $G_0$ for both actions with $\\gamma = 0.5$ and discuss which path the robot should choose to maximize its expected return.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d0d82-465f-4335-8a12-8c852937fbf4",
   "metadata": {},
   "source": [
    "# Solution \n",
    "\n",
    "With $\\gamma = 0.5$, we calculate the expected returns for turning right and left again using the provided sequences. The calculation formula remains the same, but with a modified discount factor:\n",
    "\n",
    "- For the **Right Path** ($G^{tr}_0$) with $\\gamma = 0.5$:\n",
    "\n",
    "$$\n",
    "G^{tr}_0 = R^{tr}_1 + 0.5 \\times R^{tr}_2 + 0.5^2 \\times R^{tr}_3 + \\ldots + 0.5^{11} \\times R^{tr}_{12}\n",
    "$$\n",
    "\n",
    "Plugging in the values:\n",
    "\n",
    "$$\n",
    "G^{tr}_0 = 10 + 0.5 \\times 10 + 0.5^2 \\times 10 + 0.5^3 \\times 0 + \\ldots + 0.5^7 \\times (-50) + 0.5^8 \\times 0 + \\ldots + 0.5^{11} \\times 0\n",
    "$$\n",
    "\n",
    "After calculation, this gives:\n",
    "\n",
    "$$\n",
    "G^{tr}_0 = 17.11\n",
    "$$\n",
    "\n",
    "- For the **Left Path** ($G^{tl}_0$) with $\\gamma = 0.5$:\n",
    "\n",
    "$$\n",
    "G^{tl}_0 = R^{tl}_1 + 0.5 \\times R^{tl}_2 + 0.5^2 \\times R^{tl}_3 + \\ldots + 0.5^{11} \\times R^{tl}_{12}\n",
    "$$\n",
    "\n",
    "Plugging in the values:\n",
    "\n",
    "$$\n",
    "G^{tl}_0 = 2 + 0.5 \\times 2 + 0.5^2 \\times 2 + 0.5^3 \\times 0 + \\ldots + 0.5^{11} \\times 2\n",
    "$$\n",
    "\n",
    "After calculation, this gives:\n",
    "\n",
    "$$\n",
    "G^{tl}_0 = 3.56\n",
    "$$\n",
    "\n",
    "Given the calculated expected returns, the robot should choose the **Right Path** when $\\gamma = 0.5$ since it offers a higher expected return ($G^{tr}_0 = 17.11$) compared to the **Left Path** ($G^{tl}_0 = 3.56$). This demonstrates how the choice of action can change based on the discount factor, highlighting the impact of $\\gamma$ on decision-making in reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29831ba-7e99-4fae-a889-a83195166134",
   "metadata": {},
   "source": [
    "## Action-value function definition\n",
    "In this framework, we expand the definition of the $q^*$ that we defined in the case of bandits and say:\n",
    "\n",
    "$$ q^\\pi(s, a) = \\mathbb{E_{\\pi}}[G_t | S_t = s, A_t = a],$$\n",
    "\n",
    "which is the **action-value function** under policy $\\pi(a|s)=p(a|s)$. We denote with small q the theoretical true value and with capital Q its estimate. For the moment we will focus on the true values $q$.\n",
    "\n",
    "We will show that an equivalent expression for the action-value function is:\n",
    "\n",
    "$$ q^\\pi(s, a) = \\mathbb{E_{\\pi}}[R_{t+1} + \\gamma \\; q^\\pi(s', a') | S_t = s, A_t = a], $$\n",
    "\n",
    "where starting from state $s$ and taking action $a$ under policy $\\pi$ leads us to state $s'$, where in turn we take action $a'$. This is a fundamental relationship upon which we will build the first RL learning rule for future rewards, albeit not the typical form of the Bellman equation.\n",
    "\n",
    "Here we should note that if $s'$ is terminal, we do not take another action but only observe the reward and stop there; in practice, $q^\\pi(s', a')=0$. We also note that due to the stochasticity of the environment, there are many possible $s'$ states that we can reach with different probabilities $p(s'|s,a)$, for this we need to take an expectation.\n",
    "\n",
    "## Proof\n",
    "\n",
    "We start from the definition of the expected discounted return $G_t = R_{t+1}+ \\gamma R_{t+2}+ \\gamma^2 R_{t+3}+\\ldots$ and observe we can write it as: $G_t = R_{t+1} + \\gamma G_{t+1}.$ We recall the definition: $ q^\\pi(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a].$\n",
    "\n",
    "Then we express $q^\\pi(s, a)$ using $G_t$:\n",
    "\n",
    "$$ q^\\pi(s, a) = \\mathbb{E_{\\pi}}[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a].$$\n",
    "\n",
    "We then apply the principle of linearity of expectation:\n",
    "\n",
    "$$ q^\\pi(s, a) = \\mathbb{E_{\\pi}}[R_{t+1} | S_t = s, A_t = a] + \\gamma \\; \\mathbb{E_{\\pi}}[G_{t+1} | S_t = s, A_t = a].$$\n",
    "\n",
    "For simplicity, we will define:\n",
    "\n",
    "$$Part(1)= \\mathbb{E_{\\pi}}[R_{t+1} | S_t = s, A_t = a]$$\n",
    "\n",
    "and\n",
    "\n",
    "$$Part(2) =\\mathbb{E_{\\pi}}[G_{t+1} | S_t = s, A_t = a],$$\n",
    "\n",
    "hence:\n",
    "\n",
    "$$ q^\\pi(s, a) = Part(1)+ \\gamma Part(2).$$\n",
    "\n",
    "We will leave $Part(1)$ aside and work on $Part(2)$. The idea here is that if the expectation on $G$ was conditioned to the next state-action pair $s',a'$, then we could replace it with $q^\\pi(s',a')$. So, we will manipulate the expression conditioning to these terms.\n",
    "\n",
    "To condition on $S_{t+1}$ we use the law of total expectation:\n",
    "\n",
    "$$ Part(2)=\\; \\sum_{s'} \\; p(S_{t+1} = s' | S_t = s, A_t = a) ~ \\mathbb{E_{\\pi}}[G_{t+1} | S_t = s, A_t = a, S_{t+1} = s'].$$\n",
    "\n",
    "By using the Markovian property, we can further simplify the expression:\n",
    "\n",
    "$$ Part(2)= \\sum_{s'} \\; p(S_{t+1} = s' | S_t = s, A_t = a) \\; \\mathbb{E_{\\pi}}[G_{t+1} | S_{t+1} = s'].$$\n",
    "\n",
    "Then we condition to $A_{t+1}$ using again the law of total expectation to form the term $q^\\pi(s', a')$:\n",
    "\n",
    "$$ Part(2)= \\sum_{s',a'} ~ \\pi(a'|s') p(s' | s, a) \\; \\mathbb{E_{\\pi}}[G_{t+1} | S_{t+1} = s', A_{t+1}=a'].$$\n",
    "\n",
    "Since $\\mathbb{E_{\\pi}}[G_{t+1} | S_{t+1} = s', A_{t+1}=a'] = q^\\pi(s', a')$, we can write:\n",
    "\n",
    "$$ Part(2)= \\sum_{s', a'} \\; \\pi(a'|s') ~ p(s'| s, a) q^\\pi(s', a').$$\n",
    "\n",
    "We then recognise that:\n",
    "\n",
    "$$Part(2)=\\sum_{s', a'} \\; \\pi(a'|s') \\; p(s' | s, a) q^\\pi(s', a') = \\sum_{s', a'} \\; p(s', a' | s, a) q^\\pi(s', a')= \\mathbb{E_{\\pi}} [q^\\pi(s', a')|S_t = s, A_t = a],$$\n",
    "\n",
    "i.e., the conditional expectation of $q^\\pi(s', a')$ across all possible $s'$ and $a'$ under the MDP and policy $\\pi$.\n",
    "\n",
    "We can finally place the parts back and use the linearity of expectations to combine to:\n",
    "\n",
    "$$q^\\pi(s, a) = \\mathbb{E_{\\pi}}[R_{t+1} + \\gamma \\; q^\\pi(s', a') | S_t = s, A_t = a].$$\n",
    "\n",
    "We will use this expression to derive the Bellman optimality equation for action-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b958d",
   "metadata": {},
   "source": [
    "**Note1:** Why $\\pi(a'|s') p(s'| s, a) = p(s', a' | s, a)$? \n",
    "\n",
    "Recalling that $\\pi(a'|s')= p(a'|s')=p(a'|s',s,a)$ due to the Markovian property, we aim to show that:\n",
    "\n",
    "$$p(a'|s',s,a) \\; p(s'| s, a) = p(s', a' | s, a),$$\n",
    "\n",
    "or in general:\n",
    "\n",
    "$$p(A|B,C,D)\\; p(B|C,D) = p(A, B|C,D).$$\n",
    "\n",
    "From the definition of the conditional probabilities we have:\n",
    "\n",
    "$$p(A|B,C,D) p(B|C,D) = \\left(\\frac{p(A \\cap B \\cap C \\cap D)}{p(B \\cap C \\cap D)}\\right) \\left(\\frac{p(B \\cap C \\cap D)}{p(C \\cap D)}\\right) = \\frac{p((A \\cap B) \\cap (C \\cap D))}{p(C \\cap D)} = p(A, B|C,D).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b5958",
   "metadata": {},
   "source": [
    "**Note 2:** Why $\\mathbb{E}_{\\pi}[q^\\pi(s', a')] = \\sum_{s', a'} q^\\pi(s', a') p(s', a' | s, a)$?\n",
    "\n",
    "In reinforcement learning, $q^\\pi(s', a')$, encapsulates the expected return of the next state-action pair, starting from state $s$, taking action $a$, and following policy $\\pi$. However, from a state $s$ and action $a$, multiple $s'$ states can be reached due to environmental stochasticity. For example, a robot in state $s$ may not always end up in the same state $s'$ when moving forward due to slipping. To account for this, we calculate the expectation on $q^\\pi(s', a')$, considering all possible $s'$ states.\n",
    "\n",
    "The expectation for conditional probabilities for discrete variables is expressed as:\n",
    "\n",
    "$$\\mathbb{E}[X|Y=y] = \\sum_{x} x P(X=x|Y=y).$$\n",
    "\n",
    "In the reinforcement learning context, this translates to:\n",
    "\n",
    "$$\\mathbb{E}_{\\pi}[q^\\pi(s', a')| s,a] = \\sum_{s', a'} q^\\pi(s', a') p(s', a' | s, a),$$\n",
    "\n",
    "where $X$ is parameterized by $q^\\pi(s', a')$ representing returns, conditioned on $(s, a)$, since $s'$ and $a'$ are reached from $s$ and $a$ under policy $\\pi$. The expectation considers all possible pairs of next states $s'$ and actions $a'$. Here, $p(s', a' | s, a)$ denotes the combined probability of transitioning to $s'$ and taking action $a'$ from $s$ with action $a$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff1637",
   "metadata": {},
   "source": [
    "**Note 3:** Why when calculating the expectation can I replace a variable $X$ with a parametrised function $Q(s',a')$ and take the sum over $s',a'$?\n",
    "\n",
    "The Law of the Unconscious Statistician (LOTUS) justifies calculating the expectation of a function of a random variable without needing the function's distribution directly. Consider a random variable $X$ with a known probability distribution $P(X=x)$ and a function $g(X)$ that maps outcomes of $X$ to real numbers. According to LOTUS,  expected value of $g(X)$ is defined as the sum over all possible values of $X$, weighted by their probability:\n",
    "\n",
    "$$\\mathbb{E}[g(X)] = \\sum_{x} g(x) P(X=x),$$\n",
    "\n",
    "Applying LOTUS, considering $Q(s', a')$ as $g(X)$ parameterized by $s'$ and $a'$, and extending it in the context of conditional probabilities, we get:\n",
    "\n",
    "$$\\mathbb{E}_{\\pi}[q(s', a')] = \\sum_{s', a'} q(s', a') p(s', a' | s, a),$$\n",
    "\n",
    "where $p(s', a' | s, a)$ denotes the transition probability to $s', a'$ from $s, a$. \n",
    "For a formal proof see here: https://statproofbook.github.io/P/mean-lotus.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18655607-c5cf-45a8-bad3-076cc284eaeb",
   "metadata": {},
   "source": [
    "## Example\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.dropbox.com/scl/fi/pdjmcpcpszgf5x3zn4gjj/MDP1.png?rlkey=5o3onjifzy825iol4zxspe9u8&raw=1\" width=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ae048",
   "metadata": {},
   "source": [
    "\n",
    "Back to the analog safe scenario we discussed earlier. Opening requires turning the analog dial twice, first pointing to a number $1$ and then to a number $0$. Reward R=1 is given upon opening the safe, everywhere else the reward is 0. We will assume the dial has options for only two numbers, $0$ and $1$. How many state-action pairs are there? What are the expected state-action values of each state? The initial state is S0, the state where the agent lands after guessing the first digit correctly is S1, and the state it lands if it guesses the first digit incorrectly is S2. From S1, by guessing the second digit correctly it arrives to T1 with R=1 and all other transitions get R=0. Partially incorrect choices land to T2 and entirely incorrect choices to T3. Assume $\\gamma=0.9$ and a random policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd4e4d-78c6-46df-8a7b-e7b427b40e92",
   "metadata": {},
   "source": [
    "We will make use of:\n",
    "\n",
    "$$q^\\pi(s, a) = \\mathbb{E_{\\pi}}[R_{t+1}| S_t = s, A_t = a] + \\gamma \\mathbb{E_{\\pi}}[q^\\pi(s', a') | S_t = s, A_t = a] .$$\n",
    "\n",
    "We have six q values: $(s0,0), (s0,1), (s1,0), (s1,1), (s2,0), (s2,1)$ Terminal states have no q values, or consider q values 0. The only \"rewarded\" terminal is $T1$ so we will start with the $q(S1,A2)$ (a2=0 is the correct action here) and work bakwards. \n",
    "\n",
    "$$q^\\pi(s1, A2=0) = \\mathbb{E_{\\pi}}[1 \\; | s1, 0] + 0.9 \\; \\mathbb{E_{\\pi}}[ 0 \\;| s1, 0]=1.$$\n",
    "\n",
    "Similarily:\n",
    "\n",
    "$$q^\\pi(s1, A2=1) = \\mathbb{E_{\\pi}}[0 \\; | s1, 1] + 0.9 \\; \\mathbb{E_{\\pi}}[ 0 \\;| s1, 1]=0.$$\n",
    "\n",
    "$$q^\\pi(s2, A2=0) = \\mathbb{E_{\\pi}}[0 \\; | s2, 0] + 0.9 \\; \\mathbb{E_{\\pi}}[ 0 \\;| s2, 0]=0.$$\n",
    "\n",
    "$$q^\\pi(s2, A2=1) = \\mathbb{E_{\\pi}}[0 \\; | s2, 1] + 0.9 \\; \\mathbb{E_{\\pi}}[ 0 \\;| s2, 1]=0.$$\n",
    "\n",
    "We continue with the level before, but now we need to consider the poliy because. Say  we are at state $s0$ looking forward, and take action 1, the correct action. To calculate $q^\\pi(s1, A1=1)$, we need to consider the next state-action pair. Because the environment is deterministic, we know we will land on $s1$. However, we choose the next action $a'$ randomly. This means we should average between $q^\\pi(s1, A2=0)$ and $q^\\pi(s1, A2=1)$ to estimate $\\mathbb{E_{\\pi}}[q^\\pi(s', a') | S_t = s, A_t = a]$.\n",
    "\n",
    "for a random policy, we will end up in $s1$ but from there we take randomly either action. Randomly in this case means that:  \n",
    "\n",
    "$$\\mathbb{E_{\\pi}}[q^\\pi(s', a') | s0, 1]=\\frac{q^\\pi(s1, A2=0)+q^\\pi(s1, 1)}{2}=0.5$$\n",
    "\n",
    "$$q^\\pi(s0, 1) = \\mathbb{E_{\\pi}}[0 \\;| s0, 1]+ 0.9 \\times 0.5=0.45$$\n",
    "\n",
    "and\n",
    "\n",
    "$$q^\\pi(s0, 0) = \\mathbb{E_{\\pi}}[0 + 0.9 \\times q^\\pi(s2, 1) \\;| s0, 1]=0.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b888d-3012-4fbf-b1f2-5a47142ad5c0",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "Still on the safe problem, now whenever you turn the dial to the correct digit, there is a $10\\%$ probability that something jams and internally the dial becomes the incorrect digit. What are the expected state-action values in this case?\n",
    "\n",
    "Make use of: \n",
    "\n",
    "$$q^\\pi(s, a) = \\mathbb{E_{\\pi}}[R_{t+1} + \\gamma \\; q^\\pi(s', a') | S_t = s, A_t = a]= \\mathbb{E_{\\pi}}[R_{t+1}| S_t = s, A_t = a] + \\gamma \\mathbb{E_{\\pi}}[q^\\pi(s', a') | S_t = s, A_t = a] .$$\n",
    "\n",
    "Start from a terminal state backwards, since for states leading to terminal $q^\\pi(s', a')=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bb3f1-cadf-44ad-aacf-47116f8f4379",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.dropbox.com/scl/fi/d8ekcgsbqall0mwuuz2cz/MDP3.png?rlkey=6x459zx6r21x88xhgpem8d6j8&raw=1\" width=\"600\" />\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc16f9c-d2bf-4ad5-9199-43f819706511",
   "metadata": {},
   "source": [
    "We will, again, make use of:\n",
    "\n",
    "$$q^\\pi(s, a) = \\mathbb{E_{\\pi}}[R_{t+1}| S_t = s, A_t = a] + \\gamma \\mathbb{E_{\\pi}}[q^\\pi(s', a') | S_t = s, A_t = a] .$$ The solution is nearly identical to the example with the difference that when the agent plays the correct action 10% of the time the system acts as if it has done the wrong action. \n",
    "\n",
    "As before, we have six q values: $(s0,0), (s0,1), (s1,0), (s1,1), (s2,0), (s2,1).$ However now expectations also differ.\n",
    "\n",
    "For instance, for state $s1$ selecting the correct answer ($A2=0$), no longer gives reward 1 but only $90%$ of the time.\n",
    "\n",
    "$\\mathbb{E_{\\pi}}[r \\; | s1, 0]=0.9$\n",
    "\n",
    "$$q^\\pi(s1, A2=0) = 0.9  + 0.9 \\; \\mathbb{E_{\\pi}}[ 0 \\;| s1, 0]=0.9$$\n",
    "\n",
    "The rest remain the same apart from the state-action pairs involving $s0$.\n",
    "\n",
    "$$q^\\pi(s1, A2=1) = \\mathbb{E_{\\pi}}[0 \\; | s1, 1] + 0.9 \\; \\mathbb{E_{\\pi}}[ 0 \\;| s1, 1]=0.$$\n",
    "\n",
    "$$q^\\pi(s2, A2=0) = \\mathbb{E_{\\pi}}[0 \\; | s2, 0] + 0.9 \\; \\mathbb{E_{\\pi}}[ 0 \\;| s2, 0]=0.$$\n",
    "\n",
    "$$q^\\pi(s2, A2=1) = \\mathbb{E_{\\pi}}[0 \\; | s2, 1] + 0.9 \\; \\mathbb{E_{\\pi}}[ 0 \\;| s2, 1]=0.$$\n",
    "\n",
    "\n",
    "Now when form $s1$ we take the correct action the reward is reduced in two ways.\n",
    "\n",
    "First the expectation of the reward on the next state action pair is reduced in comparison to the stochastic environment. But also, though we are operating on a random policy, the split between doing the right action and doing the wrong action changes. Under random policy the split was equal, meaning that in $n$ attempts, we would have done $n/2$ correct actions and $n/2$ wrong actions. But out correct actions are now only interpreted as correct $0.9 \\times\\frac{ n}{2} $ while the missing $0.1 \\times\\frac{ n}{2}$ is as if we made an incorrect choise. Hence the probabilities become:\n",
    "\n",
    "Correct: $0.9 \\times\\frac{n}{2n}=0.45$ and Incorrect: $0.1 \\times\\frac{ n}{2n}+\\frac{ n}{2n}$\n",
    "\n",
    "We then write:\n",
    "\n",
    "$$\\mathbb{E_{\\pi}}[q^\\pi(s', a') | s0, 1]=q^\\pi(s1, A2=0)\\times 0.55+q^\\pi(s1, 1)\\times 0.45= 0.9 \\times 0.45=0.405$$\n",
    "\n",
    "$$q^\\pi(s0, 1) = \\mathbb{E_{\\pi}}[0 \\;| s0, 1]+ 0.9 \\times 0.405=0.36$$\n",
    "\n",
    "while nothing chanhes for the non rewarded action.\n",
    "\n",
    "$$q^\\pi(s0, 0) = \\mathbb{E_{\\pi}}[0 + 0.9 \\times q^\\pi(s2, 1) \\;| s0, 1]=0.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ab92b-ac01-4cb0-83f0-aaeb7b23d11d",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equation for Action-Value Functions\n",
    "\n",
    "Our goal in RL (Reinforcement Learning) is to maximise the Reward. We therefore seek policies that will allow us to achieve as high returns as possible. We define an optimal policy as the policy that gives the highest expected return $G$. This means that $\\pi$ is an optimal policy if $q^\\pi(s, a) \\geq q^{\\pi'}(s, a)$ for all state-action pairs. In other words:\n",
    "\n",
    "$$q^*(s,a) = \\max_{\\pi} q^{\\pi}(s,a).$$\n",
    "\n",
    "Starting from the action-value equation that we derived earlier:\n",
    "\n",
    "$$q^\\pi(s, a) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma \\; q^\\pi(s', a') \\; | \\; S_t = s, A_t = a],$$\n",
    "\n",
    "we can write the Bellman Optimality Equation for Action-Value Functions by selecting greedily the action $a'$ resulting in:\n",
    "\n",
    "$$q^*(s, a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} q^*(s', a') \\; | \\; S_t = s, A_t = a].$$\n",
    "\n",
    "This equation represents the expected return for taking an action $a$ in state $s$ and thereafter following the optimal policy. By iteratively applying this equation, we can converge to the optimal action-value function $q^*$, which in turn helps us to identify the optimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88dfceb-c2f9-47b9-92c5-6aeeb44aae89",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In the context of the bandit problem, when discussing different strategies, we highlighted the limitation that a greedy policy can lead to suboptimal solutions. The algorithmic implementation in the lab illustrates that the bandit is likely to choose more frequent but smaller rewards. Why, then, in the context of the Bellman Optimality Equation, does a greedy approach seem to be an optimal strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddce303",
   "metadata": {},
   "source": [
    "## State Value Functions \n",
    "\n",
    "It is possible to also define the value function of a state, considering the expected reward across all actions that can be taken from that state. This is a more compressed form of information, typically followed in most textbooks. The value function, $v(s)$, can be defined as the expected return for being in a state $s$ and following policy $\\pi$ thereafter. Formally, it is given by:\n",
    "\n",
    "$$\n",
    "v(s) = \\mathbb{E}[G_t | S_t = s]\n",
    "$$\n",
    "\n",
    "where $G_t$ is the return at time $t$, and the expectation is taken over the distribution of possible paths that stem from following $\\pi$, starting from $s$.\n",
    "\n",
    "We can derive the Ballman equation for state values. The value of a state $s$ under a policy $\\pi$, denoted $v_\\pi(s)$, is the expected return starting from state $s$ and following policy $\\pi$ thereafter. The Bellman equation for state values expresses this as:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$,\n",
    "- $p(s', r | s, a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ after taking action $a$ in state $s$,\n",
    "- $\\gamma$ is the discount factor, which represents the difference in importance between future rewards and present rewards.\n",
    "\n",
    "\n",
    "## Optimal State Value Functions\n",
    "\n",
    "It follows that the optimal state value function, denoted as $v^*(s)$, is defined as the maximum expected return achievable under any policy, from state $s$. Formally, it is given by $v^*(s) = \\max_{a} q^{\\pi^*}(s,a)$, where $q_{\\pi^*}(s,a)$ represents the action-value function under the optimal policy $\\pi^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b5e3c-9fa5-4bb7-ac65-eae118b6623c",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "Derive the Bellman equation for the vstate alue functions. Explain all intermediate steps. Hint: Decompose it into two parts as in the proof of the Bellman equation for state action values, and work on them separately. For the first part, expand it by using the definition of expectation, and condition on  $a$ and $s'$ using the law of total expectation. Treat the second part similarly to the proof for the state-action values. To put the two parts together, you will notice the absence of the reward variable in the expression of an expected probability in the second part; you can incorporate it and then marginalise it out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a6486f-ddf9-4428-90a5-6ffd040deff5",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Proof of the Bellman Equation for State Values\n",
    "\n",
    "#### Initial Setup\n",
    "\n",
    "Given a state $s$ and a policy $\\pi$, the value of $s$ under $\\pi$ is defined as the expected return starting from $s$ and following $\\pi$. The return $G_t$ is the sum of rewards received, discounted by $\\gamma$ at each step into the future.\n",
    "\n",
    "#### Step 1: Recursive Form of $G_t$\n",
    "\n",
    "The return at time $t$ can be expressed recursively:\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma G_{t+1}\n",
    "$$\n",
    "\n",
    "#### Step 2: Expected Return from State $s$\n",
    "\n",
    "The value function $v_\\pi(s)$ is the expected return starting from state $s$ and following policy $\\pi$:\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]\n",
    "$$\n",
    "\n",
    "#### Step 3: Decomposition Using Linearity of Expectation\n",
    "\n",
    "Using the linearity of expectation, we decompose $v_\\pi(s)$ into two components:\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} | S_t = s] + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_t = s]\n",
    "$$\n",
    "\n",
    "#### Step 4: Immediate Reward Expectation\n",
    "\n",
    "The expectation of the immediate reward $R_{t+1}$ is expressed and then expanded through conditioning on actions and next states:\n",
    "$$\n",
    "\\mathbb{E}_\\pi[R_{t+1} | S_t = s] = \\sum_r r p(r|s)\n",
    "$$\n",
    "Conditioning on action $A_t$:\n",
    "$$\n",
    "\\sum_a \\pi(a|s) \\sum_r r p(r|s,a)\n",
    "$$\n",
    "Further conditioning on the next state $S_{t+1}$ yields:\n",
    "$$\n",
    "\\sum_a \\pi(a|s) \\sum_r \\sum_{s'} r p(a|s) p(s'|s,a) p(r|s,a,s')\n",
    "$$\n",
    "Recognizing that $p(s'|s,a) p(r|s,a,s') = p(r,s'|s,a)$ simplifies to:\n",
    "$$\n",
    "\\sum_a \\pi(a|s) \\sum_{s', r} r p(r,s'|s,a)\n",
    "$$\n",
    "\n",
    "#### Step 5: Future Rewards Expectation\n",
    "\n",
    "Conditioning on action $A_t$ and then on $S_{t+1}$, while applying the law of total expectation and leveraging the Markov property:\n",
    "$$\n",
    "\\gamma \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s,a) v_\\pi(s')\n",
    "$$\n",
    "\n",
    "#### Step 6: Combine Immediate and Future Rewards\n",
    "\n",
    "Integrating both parts and treating the conditional probabilities jointly:\n",
    "$$\n",
    "v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(r,s'|s,a) [r + \\gamma v_\\pi(s')]\n",
    "$$\n",
    "\n",
    "This completes the proof of the Bellman equation for state values under policy $\\pi$, illustrating how the expected sum of immediate rewards and discounted future rewards is computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838bd69-313e-44bb-a7c0-16a4b15f24a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
