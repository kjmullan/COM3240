{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gridworld_env import get_task1_gridworld, get_task2_gridworld, get_task3_gridworld\n",
    "from utils import paint_gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Look Around GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initiating the Environment. The method get_task1_gridworld\n",
    "# initialises the environment for the task 1. Similarly there are methods to initialise the environment \n",
    "# for the other tasks\n",
    "env1 = get_task1_gridworld()\n",
    "\n",
    "# This is how the environment looks like\n",
    "paint_gridworld(env1, figsize=(4,4), ticksize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of running a simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected rewards of an episode: [0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10.0]\n"
     ]
    }
   ],
   "source": [
    "reward_lst = []\n",
    "\n",
    "# Initialise the environment\n",
    "# An arbitary gamma value is used here. You may need to change this!\n",
    "env1 = get_task1_gridworld()\n",
    "\n",
    "# Reset the environment to start from the starting location. \n",
    "# The reset() method returns the state, reward, and whether the episode is done or not\n",
    "# More information can be found under the definition of the method\n",
    "_, state, reward, done = env1.reset()\n",
    "reward_lst.append(reward)\n",
    "\n",
    "# While the trial is not done, i.e. the agent has not fallen into lava,\n",
    "# the maximum length of the episode has not been exceeded, and the target has not been reached\n",
    "while not done:\n",
    "    \n",
    "    # Take a step using the step() function and passing in one of the available actions [0, 1, 2, 3] \n",
    "    # corresponding to ['N', 'S', 'E', 'W']\n",
    "    # More information can be found under the definition of the method\n",
    "    # In this very simple example for each step we choose a random action\n",
    "    _, state, reward, done = env1.step(np.random.choice(4))\n",
    "    reward_lst.append(reward)\n",
    "\n",
    "print(f'Collected rewards of an episode: {reward_lst}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_agent():\n",
    "  # The solve method should take the n_episodes and env as a parameter, \n",
    "  # but otherwise you can modify the definition of this method\n",
    "  def solve(self, env, n_episodes=1):\n",
    "    \"\"\"\n",
    "    Solve a given GridWorld environment using SARSA\n",
    "    input: env {GridWorld object} -- GridWorld to solve\n",
    "    output:\n",
    "      - policy {np.array} -- Optimal policy found to solve the given GridWorld environment\n",
    "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each trial\n",
    "      - total_steps {list of float} -- Corresponding list of successive total non-discounted sum of steps for each trial\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise your variables\n",
    "\n",
    "    ####\n",
    "    # Add your code here\n",
    "    # WARNING: this agent only has access to env.reset() and env.step()\n",
    "    # You should not use env.get_absorbing() to compute any value\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # You may use these outputs to plot graphs\n",
    "    return policy, total_rewards, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA_agent():\n",
    "  # The solve method should take the n_episodes and env as a parameter, \n",
    "  # but otherwise you can modify the definition of this method\n",
    "  def solve(self, env, n_episodes=500, \n",
    "             # Extra parameters:\n",
    "            gamma=0.1, alpha=0.1, epsilon=0.1, ):\n",
    "    \"\"\"\n",
    "    Solve a given GridWorld environment using SARSA\n",
    "    input: env {GridWorld object} -- GridWorld to solve\n",
    "    output:\n",
    "      - policy {np.array} -- Optimal policy found to solve the given GridWorld environment\n",
    "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each trial\n",
    "      - total_steps {list of float} -- Corresponding list of successive total non-discounted sum of steps for each trial\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise your variables\n",
    "\n",
    "    ####\n",
    "    # Add your code here\n",
    "    # WARNING: this agent only has access to env.reset() and env.step()\n",
    "    # You should not use env.get_absorbing() to compute any value\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    # You may use these outputs to plot graphs\n",
    "    return policy, total_rewards, total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Single Target, Deterministic Rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Hyperparameters\n",
    "# This is just an example, you can add to this depending on which method you utilise\n",
    "\n",
    "gamma = 0.3\n",
    "# Alpha === Learning Rate\n",
    "alpha = 0.5\n",
    "epsilon = 0.6\n",
    "n_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the definition of the gridworld for task 1. DO NOT CHANGE THIS\n",
    "env1 = get_task1_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example of running SARSA\u001b[39;00m\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m SARSA_agent()\n\u001b[1;32m----> 3\u001b[0m policy, total_rewards, total_steps \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mSARSA_agent.solve\u001b[1;34m(self, env, n_episodes, gamma, alpha, epsilon)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mSolve a given GridWorld environment using SARSA\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03minput: env {GridWorld object} -- GridWorld to solve\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m  - total_steps {list of float} -- Corresponding list of successive total non-discounted sum of steps for each trial\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Initialise your variables\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m####\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# You may use these outputs to plot graphs\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpolicy\u001b[49m, total_rewards, total_steps\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "# Example of running SARSA\n",
    "agent = SARSA_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env1, gamma=gamma, alpha=alpha, epsilon=epsilon, n_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example of running Q-Learning\u001b[39;00m\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m Q_agent()\n\u001b[1;32m----> 3\u001b[0m policy, total_rewards, total_steps \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m, in \u001b[0;36mQ_agent.solve\u001b[1;34m(self, env, n_episodes)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mSolve a given GridWorld environment using SARSA\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03minput: env {GridWorld object} -- GridWorld to solve\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m  - total_steps {list of float} -- Corresponding list of successive total non-discounted sum of steps for each trial\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialise your variables\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m####\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# You may use these outputs to plot graphs\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpolicy\u001b[49m, total_rewards, total_steps\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "# Example of running Q-Learning\n",
    "agent = Q_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your plots below! Include any new function definitions in this jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Dual Targets with Stochastic Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the definition of the gridworld for task 1. DO NOT CHANGE THIS\n",
    "env2 = get_task2_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running SARSA\n",
    "agent = SARSA_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running Q-Learning\n",
    "agent = Q_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your plots below! Include any new function definitions in this jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Stochastic Transitions and Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the definition of the gridworld for task 1. DO NOT CHANGE THIS\n",
    "env3 = get_task3_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running SARSA\n",
    "agent = SARSA_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running Q-Learning\n",
    "agent = Q_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your plots below! Include any new function definitions in this jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
