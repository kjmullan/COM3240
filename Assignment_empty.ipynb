{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Zyww5Q6zFIM9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gridworld_env import get_task1_gridworld, get_task2_gridworld, get_task3_gridworld\n",
    "from utils import paint_gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbY8DCqoVJlw"
   },
   "source": [
    "## A Look Around GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "CuEktkbnFINB",
    "outputId": "058f3b07-0dbc-40f4-cf31-10b9aed8974d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initiating the Environment. The method get_task1_gridworld\n",
    "# initialises the environment for the task 1. Similarly there are methods to initialise the environment \n",
    "# for the other tasks\n",
    "env1 = get_task1_gridworld()\n",
    "\n",
    "# This is how the environment looks like\n",
    "paint_gridworld(env1, figsize=(4,4), ticksize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of running a simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected rewards of an episode: [0, 0.0, 0.0, 0.0, 0.0, -10.0]\n"
     ]
    }
   ],
   "source": [
    "reward_lst = []\n",
    "\n",
    "# Initialise the environment\n",
    "# An arbitary gamma value is used here. You may need to change this!\n",
    "env1 = get_task1_gridworld()\n",
    "\n",
    "# Reset the environment to start from the starting location. \n",
    "# The reset() method returns the state, reward, and whether the episode is done or not\n",
    "# More information can be found under the definition of the method\n",
    "_, state, reward, done = env1.reset()\n",
    "reward_lst.append(reward)\n",
    "\n",
    "# While the trial is not done, i.e. the agent has not fallen into lava,\n",
    "# the maximum length of the episode has not been exceeded, and the target has not been reached\n",
    "while not done:\n",
    "    \n",
    "    # Take a step using the step() function and passing in one of the available actions [0, 1, 2, 3] \n",
    "    # corresponding to ['N', 'S', 'E', 'W']\n",
    "    # More information can be found under the definition of the method\n",
    "    # In this very simple example for each step we choose a random action\n",
    "    _, state, reward, done = env1.step(np.random.choice(4))\n",
    "    reward_lst.append(reward)\n",
    "\n",
    "print(f'Collected rewards of an episode: {reward_lst}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_agent():\n",
    "  # The solve method should take the n_episodes and env as a parameter, \n",
    "  # but otherwise you can modify the definition of this method\n",
    "  def solve(self, env, n_episodes=1):\n",
    "    \"\"\"\n",
    "    Solve a given GridWorld environment using SARSA\n",
    "    input: env {GridWorld object} -- GridWorld to solve\n",
    "    output:\n",
    "      - policy {np.array} -- Optimal policy found to solve the given GridWorld environment\n",
    "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each trial\n",
    "      - total_steps {list of float} -- Corresponding list of successive total non-discounted sum of steps for each trial\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise your variables\n",
    "\n",
    "    ####\n",
    "    # Add your code here\n",
    "    # WARNING: this agent only has access to env.reset() and env.step()\n",
    "    # You should not use env.get_absorbing() to compute any value\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # You may use these outputs to plot graphs\n",
    "    return policy, total_rewards, total_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA_agent():\n",
    "  # The solve method should take the n_episodes and env as a parameter, \n",
    "  # but otherwise you can modify the definition of this method\n",
    "  def solve(self, env, n_episodes=500, \n",
    "             # Extra parameters:\n",
    "            gamma=0.1, alpha=0.1, epsilon=0.1, ):\n",
    "    \"\"\"\n",
    "    Solve a given GridWorld environment using SARSA\n",
    "    input: env {GridWorld object} -- GridWorld to solve\n",
    "    output:\n",
    "      - policy {np.array} -- Optimal policy found to solve the given GridWorld environment\n",
    "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each trial\n",
    "      - total_steps {list of float} -- Corresponding list of successive total non-discounted sum of steps for each trial\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise your variables\n",
    "\n",
    "    ####\n",
    "    # Add your code here\n",
    "    # WARNING: this agent only has access to env.reset() and env.step()\n",
    "    # You should not use env.get_absorbing() to compute any value\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    # You may use these outputs to plot graphs\n",
    "    return policy, total_rewards, total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e96UhvEwFINC"
   },
   "source": [
    "# Task 1 - Single Target, Deterministic Rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Hyperparameters\n",
    "# This is just an example, you can add to this depending on which method you utilise\n",
    "\n",
    "gamma = 0.3\n",
    "# Alpha === Learning Rate\n",
    "alpha = 0.5\n",
    "epsilon = 0.6\n",
    "n_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the definition of the gridworld for task 1. DO NOT CHANGE THIS\n",
    "env1 = get_task1_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running SARSA\n",
    "agent = SARSA_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env1, gamma=gamma, alpha=alpha, epsilon=epsilon, n_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running Q-Learning\n",
    "agent = Q_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLyna1D6FINC"
   },
   "outputs": [],
   "source": [
    "# Do your plots below! Include any new function definitions in this jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkFQmaRaFINC"
   },
   "source": [
    "# Task 2 - Dual Targets with Stochastic Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the definition of the gridworld for task 1. DO NOT CHANGE THIS\n",
    "env2 = get_task2_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running SARSA\n",
    "agent = SARSA_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running Q-Learning\n",
    "agent = Q_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your plots below! Include any new function definitions in this jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J48sF1qHFIND"
   },
   "source": [
    "# Task 3 - Stochastic Transitions and Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the definition of the gridworld for task 1. DO NOT CHANGE THIS\n",
    "env3 = get_task3_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running SARSA\n",
    "agent = SARSA_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running Q-Learning\n",
    "agent = Q_agent()\n",
    "policy, total_rewards, total_steps = agent.solve(env3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your plots below! Include any new function definitions in this jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
